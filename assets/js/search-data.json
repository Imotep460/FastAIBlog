{
  
    
        "post0": {
            "title": "Title",
            "content": ". Fast AI Image clasification - Kapitel 5 - 09-03-2021 . Fast Ai - kapitel 5 . !pip install -Uqq fastbook import fastbook fastbook.setup_book() . ERROR: torchtext 0.9.0 has requirement torch==1.8.0, but you&#39;ll have torch 1.7.1 which is incompatible. Mounted at /content/gdrive . from fastbook import * . from fastai.vision.all import * path = untar_data(URLs.PETS) . Path.BASE_PATH = path . path.ls() . (#2) [Path(&#39;annotations&#39;),Path(&#39;images&#39;)] . (path/&quot;images&quot;).ls() . (#7393) [Path(&#39;images/Birman_183.jpg&#39;),Path(&#39;images/Sphynx_125.jpg&#39;),Path(&#39;images/Bombay_117.jpg&#39;),Path(&#39;images/newfoundland_65.jpg&#39;),Path(&#39;images/British_Shorthair_31.jpg&#39;),Path(&#39;images/Ragdoll_252.jpg&#39;),Path(&#39;images/basset_hound_70.jpg&#39;),Path(&#39;images/Persian_30.jpg&#39;),Path(&#39;images/pomeranian_137.jpg&#39;),Path(&#39;images/Bengal_130.jpg&#39;)...] . fname = (path/&quot;images&quot;).ls()[0] . re.findall(r&#39;(.+)_ d+.jpg$&#39;, fname.name) . [&#39;Birman&#39;] . pets = DataBlock(blocks = (ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(seed=42), get_y=using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;), item_tfms=Resize(460), batch_tfms=aug_transforms(size=224, min_scale=0.75)) dls = pets.dataloaders(path/&quot;images&quot;) . #id interpolations #caption A comparison of fastai&#39;s data augmentation strategy (left) and the traditional approach (right). dblock1 = DataBlock(blocks=(ImageBlock(), CategoryBlock()), get_y=parent_label, item_tfms=Resize(460)) # Place an image in the &#39;images/grizzly.jpg&#39; subfolder where this notebook is located before running this dls1 = dblock1.dataloaders([(Path.cwd()/&#39;images&#39;/&#39;miniature_pinscher.jpg&#39;)]*100, bs=8) dls1.train.get_idxs = lambda: Inf.ones x,y = dls1.valid.one_batch() _,axs = subplots(1, 2) x1 = TensorImage(x.clone()) x1 = x1.affine_coord(sz=224) x1 = x1.rotate(draw=30, p=1.) x1 = x1.zoom(draw=1.2, p=1.) x1 = x1.warp(draw_x=-0.2, draw_y=0.2, p=1.) tfms = setup_aug_tfms([Rotate(draw=30, p=1, size=224), Zoom(draw=1.2, p=1., size=224), Warp(draw_x=-0.2, draw_y=0.2, p=1., size=224)]) x = Pipeline(tfms)(x) #x.affine_coord(coord_tfm=coord_tfm, sz=size, mode=mode, pad_mode=pad_mode) TensorImage(x[0]).show(ctx=axs[0]) TensorImage(x1[0]).show(ctx=axs[1]); . dls.show_batch(nrows=1, ncols=3) . pets1 = DataBlock(blocks = (ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(seed=42), get_y=using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;), item_tfms=Resize(460), batch_tfms=aug_transforms(size=224, min_scale=0.75)) pets1.summary(path/&quot;images&quot;) . Setting-up type transforms pipelines Collecting items from /root/.fastai/data/oxford-iiit-pet/images Found 7390 items 2 datasets of sizes 5912,1478 Setting up Pipeline: PILBase.create Setting up Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} Building one sample Pipeline: PILBase.create starting from /root/.fastai/data/oxford-iiit-pet/images/basset_hound_51.jpg applying PILBase.create gives PILImage mode=RGB size=333x500 Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} starting from /root/.fastai/data/oxford-iiit-pet/images/basset_hound_51.jpg applying partial gives basset_hound applying Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} gives TensorCategory(14) Final sample: (PILImage mode=RGB size=333x500, TensorCategory(14)) Collecting items from /root/.fastai/data/oxford-iiit-pet/images Found 7390 items 2 datasets of sizes 5912,1478 Setting up Pipeline: PILBase.create Setting up Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} Setting up after_item: Pipeline: Resize -- {&#39;size&#39;: (460, 460), &#39;method&#39;: &#39;crop&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;resamples&#39;: (2, 0), &#39;p&#39;: 1.0} -&gt; ToTensor Setting up before_batch: Pipeline: Setting up after_batch: Pipeline: IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} -&gt; Flip -- {&#39;size&#39;: None, &#39;mode&#39;: &#39;bilinear&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;mode_mask&#39;: &#39;nearest&#39;, &#39;align_corners&#39;: True, &#39;p&#39;: 0.5} -&gt; RandomResizedCropGPU -- {&#39;size&#39;: (224, 224), &#39;min_scale&#39;: 0.75, &#39;ratio&#39;: (1, 1), &#39;mode&#39;: &#39;bilinear&#39;, &#39;valid_scale&#39;: 1.0, &#39;p&#39;: 1.0} -&gt; Brightness -- {&#39;max_lighting&#39;: 0.2, &#39;p&#39;: 1.0, &#39;draw&#39;: None, &#39;batch&#39;: False} Building one batch Applying item_tfms to the first sample: Pipeline: Resize -- {&#39;size&#39;: (460, 460), &#39;method&#39;: &#39;crop&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;resamples&#39;: (2, 0), &#39;p&#39;: 1.0} -&gt; ToTensor starting from (PILImage mode=RGB size=333x500, TensorCategory(14)) applying Resize -- {&#39;size&#39;: (460, 460), &#39;method&#39;: &#39;crop&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;resamples&#39;: (2, 0), &#39;p&#39;: 1.0} gives (PILImage mode=RGB size=460x460, TensorCategory(14)) applying ToTensor gives (TensorImage of size 3x460x460, TensorCategory(14)) Adding the next 3 samples No before_batch transform to apply Collating items in a batch Applying batch_tfms to the batch built Pipeline: IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} -&gt; Flip -- {&#39;size&#39;: None, &#39;mode&#39;: &#39;bilinear&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;mode_mask&#39;: &#39;nearest&#39;, &#39;align_corners&#39;: True, &#39;p&#39;: 0.5} -&gt; RandomResizedCropGPU -- {&#39;size&#39;: (224, 224), &#39;min_scale&#39;: 0.75, &#39;ratio&#39;: (1, 1), &#39;mode&#39;: &#39;bilinear&#39;, &#39;valid_scale&#39;: 1.0, &#39;p&#39;: 1.0} -&gt; Brightness -- {&#39;max_lighting&#39;: 0.2, &#39;p&#39;: 1.0, &#39;draw&#39;: None, &#39;batch&#39;: False} starting from (TensorImage of size 4x3x460x460, TensorCategory([14, 1, 6, 34], device=&#39;cuda:0&#39;)) applying IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} gives (TensorImage of size 4x3x460x460, TensorCategory([14, 1, 6, 34], device=&#39;cuda:0&#39;)) applying Flip -- {&#39;size&#39;: None, &#39;mode&#39;: &#39;bilinear&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;mode_mask&#39;: &#39;nearest&#39;, &#39;align_corners&#39;: True, &#39;p&#39;: 0.5} gives (TensorImage of size 4x3x460x460, TensorCategory([14, 1, 6, 34], device=&#39;cuda:0&#39;)) applying RandomResizedCropGPU -- {&#39;size&#39;: (224, 224), &#39;min_scale&#39;: 0.75, &#39;ratio&#39;: (1, 1), &#39;mode&#39;: &#39;bilinear&#39;, &#39;valid_scale&#39;: 1.0, &#39;p&#39;: 1.0} gives (TensorImage of size 4x3x224x224, TensorCategory([14, 1, 6, 34], device=&#39;cuda:0&#39;)) applying Brightness -- {&#39;max_lighting&#39;: 0.2, &#39;p&#39;: 1.0, &#39;draw&#39;: None, &#39;batch&#39;: False} gives (TensorImage of size 4x3x224x224, TensorCategory([14, 1, 6, 34], device=&#39;cuda:0&#39;)) . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(2) . Downloading: &#34;https://download.pytorch.org/models/resnet34-333f7ec4.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth . . epoch train_loss valid_loss error_rate time . 0 | 1.463172 | 1.101596 | 0.339648 | 01:55 | . epoch train_loss valid_loss error_rate time . 0 | 0.507639 | 1.001056 | 0.294317 | 02:27 | . 1 | 0.330711 | 0.841297 | 0.254398 | 02:27 | . x,y = dls.one_batch() . y . TensorCategory([ 7, 7, 0, 10, 5, 12, 19, 0, 24, 35, 6, 19, 33, 9, 27, 27, 34, 13, 11, 34, 11, 19, 18, 35, 31, 33, 34, 27, 4, 32, 34, 19, 12, 16, 17, 0, 11, 23, 29, 13, 25, 14, 36, 31, 5, 29, 12, 23, 22, 28, 4, 30, 25, 16, 17, 20, 24, 8, 4, 27, 34, 25, 19, 11], device=&#39;cuda:0&#39;) . preds,_ = learn.get_preds(dl=[(x,y)]) preds[0] . tensor([1.3026e-08, 4.1057e-08, 2.1229e-06, 1.3474e-08, 5.7301e-06, 8.5949e-09, 2.8635e-04, 9.9957e-01, 1.0431e-05, 8.2644e-08, 3.1241e-09, 3.8788e-08, 3.1625e-08, 3.9096e-08, 3.8911e-08, 4.1528e-08, 2.3954e-08, 5.3725e-09, 6.2817e-08, 1.8606e-08, 3.3789e-08, 2.4073e-07, 1.0377e-04, 7.2735e-07, 6.6701e-07, 6.4277e-07, 7.1024e-09, 4.2683e-07, 1.2703e-05, 3.6499e-08, 4.7538e-08, 1.7708e-06, 7.3165e-08, 2.0073e-07, 1.9836e-08, 4.8690e-07, 6.4921e-07]) . len(preds[0]),preds[0].sum() . (37, tensor(1.0000)) . plot_function(torch.sigmoid, min=-4,max=4) . /usr/local/lib/python3.7/dist-packages/fastbook/__init__.py:73: UserWarning: Not providing a value for linspace&#39;s steps is deprecated and will throw a runtime error in a future release. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/RangeFactories.cpp:23.) x = torch.linspace(min,max) . torch.random.manual_seed(42); . acts = torch.randn((6,2))*2 acts . tensor([[ 0.6734, 0.2576], [ 0.4689, 0.4607], [-2.2457, -0.3727], [ 4.4164, -1.2760], [ 0.9233, 0.5347], [ 1.0698, 1.6187]]) . acts.sigmoid() . tensor([[0.6623, 0.5641], [0.6151, 0.6132], [0.0957, 0.4079], [0.9881, 0.2182], [0.7157, 0.6306], [0.7446, 0.8346]]) . (acts[:,0]-acts[:,1]).sigmoid() . tensor([0.6025, 0.5021, 0.1332, 0.9966, 0.5959, 0.3661]) . sm_acts = torch.softmax(acts, dim=1) sm_acts . tensor([[0.6025, 0.3975], [0.5021, 0.4979], [0.1332, 0.8668], [0.9966, 0.0034], [0.5959, 0.4041], [0.3661, 0.6339]]) . targ = tensor([0,1,0,1,1,0]) . sm_acts . tensor([[0.6025, 0.3975], [0.5021, 0.4979], [0.1332, 0.8668], [0.9966, 0.0034], [0.5959, 0.4041], [0.3661, 0.6339]]) . idx = range(6) sm_acts[idx, targ] . tensor([0.6025, 0.4979, 0.1332, 0.0034, 0.4041, 0.3661]) . from IPython.display import HTML df = pd.DataFrame(sm_acts, columns=[&quot;3&quot;,&quot;7&quot;]) df[&#39;targ&#39;] = targ df[&#39;idx&#39;] = idx df[&#39;loss&#39;] = sm_acts[range(6), targ] t = df.style.hide_index() #To have html code compatible with our script html = t._repr_html_().split(&#39;&lt;/style&gt;&#39;)[1] html = re.sub(r&#39;&lt;table id=&quot;([^&quot;]+)&quot; s*&gt;&#39;, r&#39;&lt;table &gt;&#39;, html) display(HTML(html)) . 3 7 targ idx loss . 0.602469 | 0.397531 | 0 | 0 | 0.602469 | . 0.502065 | 0.497935 | 1 | 1 | 0.497935 | . 0.133188 | 0.866811 | 0 | 2 | 0.133188 | . 0.996640 | 0.003360 | 1 | 3 | 0.003360 | . 0.595949 | 0.404051 | 1 | 4 | 0.404051 | . 0.366118 | 0.633882 | 0 | 5 | 0.366118 | . -sm_acts[idx, targ] . tensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661]) . F.nll_loss(sm_acts, targ, reduction=&#39;none&#39;) . tensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661]) . plot_function(torch.log, min=0,max=4) . loss_func = nn.CrossEntropyLoss() . F.cross_entropy(acts, targ) . tensor(1.8045) . nn.CrossEntropyLoss(reduction=&#39;none&#39;)(acts, targ) . tensor([0.5067, 0.6973, 2.0160, 5.6958, 0.9062, 1.0048]) . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix(figsize=(12,12), dpi=60) . interp.most_confused(min_val=5) . [(&#39;Maine_Coon&#39;, &#39;Ragdoll&#39;, 15), (&#39;Russian_Blue&#39;, &#39;British_Shorthair&#39;, 13), (&#39;Birman&#39;, &#39;Ragdoll&#39;, 12), (&#39;beagle&#39;, &#39;basset_hound&#39;, 11), (&#39;yorkshire_terrier&#39;, &#39;havanese&#39;, 10), (&#39;staffordshire_bull_terrier&#39;, &#39;american_pit_bull_terrier&#39;, 9), (&#39;Siamese&#39;, &#39;Birman&#39;, 8), (&#39;english_setter&#39;, &#39;english_cocker_spaniel&#39;, 8), (&#39;keeshond&#39;, &#39;samoyed&#39;, 8), (&#39;Maine_Coon&#39;, &#39;Persian&#39;, 7), (&#39;staffordshire_bull_terrier&#39;, &#39;american_bulldog&#39;, 7), (&#39;wheaten_terrier&#39;, &#39;havanese&#39;, 7), (&#39;Siamese&#39;, &#39;Bombay&#39;, 6), (&#39;chihuahua&#39;, &#39;miniature_pinscher&#39;, 6), (&#39;english_setter&#39;, &#39;basset_hound&#39;, 6), (&#39;Abyssinian&#39;, &#39;Sphynx&#39;, 5), (&#39;american_bulldog&#39;, &#39;american_pit_bull_terrier&#39;, 5), (&#39;beagle&#39;, &#39;american_bulldog&#39;, 5), (&#39;chihuahua&#39;, &#39;Sphynx&#39;, 5), (&#39;leonberger&#39;, &#39;newfoundland&#39;, 5), (&#39;saint_bernard&#39;, &#39;american_bulldog&#39;, 5)] . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(1, base_lr=0.1) . epoch train_loss valid_loss error_rate time . 0 | 2.641252 | 10.990454 | 0.837618 | 01:56 | . epoch train_loss valid_loss error_rate time . 0 | 3.699024 | 3.996556 | 0.953992 | 02:29 | . learn = cnn_learner(dls, resnet34, metrics=error_rate) lr_min,lr_steep = learn.lr_find() . print(f&quot;Minimum/10: {lr_min:.2e}, steepest point: {lr_steep:.2e}&quot;) . Minimum/10: 1.00e-02, steepest point: 3.63e-03 . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(2, base_lr=3e-3) . epoch train_loss valid_loss error_rate time . 0 | 1.347023 | 0.981549 | 0.288227 | 01:55 | . epoch train_loss valid_loss error_rate time . 0 | 0.566394 | 1.174068 | 0.330176 | 02:29 | . 1 | 0.334773 | 0.752081 | 0.225304 | 02:29 | . learn.fine_tune?? . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fit_one_cycle(3, 3e-3) . epoch train_loss valid_loss error_rate time . 0 | 1.104290 | 0.933496 | 0.271989 | 01:56 | . 1 | 0.547801 | 0.765140 | 0.230717 | 01:55 | . 2 | 0.327906 | 0.682352 | 0.209743 | 01:55 | . learn.unfreeze() . learn.lr_find() . SuggestedLRs(lr_min=9.999999747378752e-07, lr_steep=2.2908675418875646e-06) . learn.fit_one_cycle(6, lr_max=1e-5) . epoch train_loss valid_loss error_rate time . 0 | 0.258392 | 0.652376 | 0.200947 | 02:27 | . 1 | 0.242147 | 0.618082 | 0.189445 | 02:27 | . 2 | 0.231463 | 0.622283 | 0.196888 | 02:27 | . 3 | 0.204535 | 0.595082 | 0.188092 | 02:27 | . 4 | 0.180945 | 0.592548 | 0.186062 | 02:27 | . 5 | 0.171708 | 0.603632 | 0.192828 | 02:28 | . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fit_one_cycle(3, 3e-3) learn.unfreeze() learn.fit_one_cycle(12, lr_max=slice(1e-6,1e-4)) . epoch train_loss valid_loss error_rate time . 0 | 1.124157 | 0.991350 | 0.308525 | 01:55 | . 1 | 0.527673 | 0.893586 | 0.282138 | 01:55 | . 2 | 0.339577 | 0.801603 | 0.253721 | 01:54 | . epoch train_loss valid_loss error_rate time . 0 | 0.272625 | 0.764421 | 0.238836 | 02:26 | . 1 | 0.260812 | 0.761544 | 0.242896 | 02:26 | . 2 | 0.229428 | 0.696853 | 0.222598 | 02:26 | . 3 | 0.226262 | 0.675862 | 0.219215 | 02:29 | . 4 | 0.189527 | 0.686940 | 0.220568 | 02:29 | . 5 | 0.173371 | 0.682683 | 0.218539 | 02:29 | . 6 | 0.165010 | 0.650099 | 0.208390 | 02:29 | . 7 | 0.161229 | 0.651157 | 0.209743 | 02:29 | . 8 | 0.138687 | 0.640442 | 0.203654 | 02:29 | . 9 | 0.127961 | 0.651706 | 0.202300 | 02:29 | . 10 | 0.124051 | 0.615696 | 0.193505 | 02:29 | . 11 | 0.124554 | 0.640680 | 0.200947 | 02:29 | . learn.recorder.plot_loss() . from fastai.callback.fp16 import * learn = cnn_learner(dls, resnet50, metrics=error_rate).to_fp16() learn.fine_tune(6, freeze_epochs=3) . Downloading: &#34;https://download.pytorch.org/models/resnet50-19c8e357.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth . . epoch train_loss valid_loss error_rate time . 0 | 1.264476 | 1.317010 | 0.381597 | 02:17 | . 1 | 0.602886 | 1.197397 | 0.348444 | 02:16 | . 2 | 0.430401 | 1.302739 | 0.358593 | 02:16 | . epoch train_loss valid_loss error_rate time . 0 | 0.267705 | 1.193918 | 0.342355 | 02:57 | . 1 | 0.333476 | 1.369267 | 0.364005 | 02:57 | . 2 | 0.258500 | 1.029740 | 0.303112 | 02:57 | . 3 | 0.147498 | 0.883164 | 0.253721 | 02:57 | . 4 | 0.085893 | 0.760042 | 0.224628 | 02:57 | . 5 | 0.052952 | 0.767537 | 0.227334 | 02:56 | .",
            "url": "https://imotep460.github.io/FastAIBlog/2021/03/09/Kapitel_5_FastAI.html",
            "relUrl": "/2021/03/09/Kapitel_5_FastAI.html",
            "date": " • Mar 9, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Title",
            "content": ". Fast AI Image clasification - Kapitel 5 - 09-03-2021 . !pip install -Uqq fastbook import fastbook fastbook.setup_book() . ERROR: torchtext 0.9.0 has requirement torch==1.8.0, but you&#39;ll have torch 1.7.1 which is incompatible. Mounted at /content/gdrive . from fastbook import * . from fastai.vision.all import * path = untar_data(URLs.PETS) . Path.BASE_PATH = path . path.ls() . (#2) [Path(&#39;annotations&#39;),Path(&#39;images&#39;)] . (path/&quot;images&quot;).ls() . (#7393) [Path(&#39;images/Birman_183.jpg&#39;),Path(&#39;images/Sphynx_125.jpg&#39;),Path(&#39;images/Bombay_117.jpg&#39;),Path(&#39;images/newfoundland_65.jpg&#39;),Path(&#39;images/British_Shorthair_31.jpg&#39;),Path(&#39;images/Ragdoll_252.jpg&#39;),Path(&#39;images/basset_hound_70.jpg&#39;),Path(&#39;images/Persian_30.jpg&#39;),Path(&#39;images/pomeranian_137.jpg&#39;),Path(&#39;images/Bengal_130.jpg&#39;)...] . fname = (path/&quot;images&quot;).ls()[0] . re.findall(r&#39;(.+)_ d+.jpg$&#39;, fname.name) . [&#39;Birman&#39;] . pets = DataBlock(blocks = (ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(seed=42), get_y=using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;), item_tfms=Resize(460), batch_tfms=aug_transforms(size=224, min_scale=0.75)) dls = pets.dataloaders(path/&quot;images&quot;) . #id interpolations #caption A comparison of fastai&#39;s data augmentation strategy (left) and the traditional approach (right). dblock1 = DataBlock(blocks=(ImageBlock(), CategoryBlock()), get_y=parent_label, item_tfms=Resize(460)) # Place an image in the &#39;images/grizzly.jpg&#39; subfolder where this notebook is located before running this dls1 = dblock1.dataloaders([(Path.cwd()/&#39;images&#39;/&#39;miniature_pinscher.jpg&#39;)]*100, bs=8) dls1.train.get_idxs = lambda: Inf.ones x,y = dls1.valid.one_batch() _,axs = subplots(1, 2) x1 = TensorImage(x.clone()) x1 = x1.affine_coord(sz=224) x1 = x1.rotate(draw=30, p=1.) x1 = x1.zoom(draw=1.2, p=1.) x1 = x1.warp(draw_x=-0.2, draw_y=0.2, p=1.) tfms = setup_aug_tfms([Rotate(draw=30, p=1, size=224), Zoom(draw=1.2, p=1., size=224), Warp(draw_x=-0.2, draw_y=0.2, p=1., size=224)]) x = Pipeline(tfms)(x) #x.affine_coord(coord_tfm=coord_tfm, sz=size, mode=mode, pad_mode=pad_mode) TensorImage(x[0]).show(ctx=axs[0]) TensorImage(x1[0]).show(ctx=axs[1]); . dls.show_batch(nrows=1, ncols=3) . pets1 = DataBlock(blocks = (ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(seed=42), get_y=using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;), item_tfms=Resize(460), batch_tfms=aug_transforms(size=224, min_scale=0.75)) pets1.summary(path/&quot;images&quot;) . Setting-up type transforms pipelines Collecting items from /root/.fastai/data/oxford-iiit-pet/images Found 7390 items 2 datasets of sizes 5912,1478 Setting up Pipeline: PILBase.create Setting up Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} Building one sample Pipeline: PILBase.create starting from /root/.fastai/data/oxford-iiit-pet/images/basset_hound_51.jpg applying PILBase.create gives PILImage mode=RGB size=333x500 Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} starting from /root/.fastai/data/oxford-iiit-pet/images/basset_hound_51.jpg applying partial gives basset_hound applying Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} gives TensorCategory(14) Final sample: (PILImage mode=RGB size=333x500, TensorCategory(14)) Collecting items from /root/.fastai/data/oxford-iiit-pet/images Found 7390 items 2 datasets of sizes 5912,1478 Setting up Pipeline: PILBase.create Setting up Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} Setting up after_item: Pipeline: Resize -- {&#39;size&#39;: (460, 460), &#39;method&#39;: &#39;crop&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;resamples&#39;: (2, 0), &#39;p&#39;: 1.0} -&gt; ToTensor Setting up before_batch: Pipeline: Setting up after_batch: Pipeline: IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} -&gt; Flip -- {&#39;size&#39;: None, &#39;mode&#39;: &#39;bilinear&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;mode_mask&#39;: &#39;nearest&#39;, &#39;align_corners&#39;: True, &#39;p&#39;: 0.5} -&gt; RandomResizedCropGPU -- {&#39;size&#39;: (224, 224), &#39;min_scale&#39;: 0.75, &#39;ratio&#39;: (1, 1), &#39;mode&#39;: &#39;bilinear&#39;, &#39;valid_scale&#39;: 1.0, &#39;p&#39;: 1.0} -&gt; Brightness -- {&#39;max_lighting&#39;: 0.2, &#39;p&#39;: 1.0, &#39;draw&#39;: None, &#39;batch&#39;: False} Building one batch Applying item_tfms to the first sample: Pipeline: Resize -- {&#39;size&#39;: (460, 460), &#39;method&#39;: &#39;crop&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;resamples&#39;: (2, 0), &#39;p&#39;: 1.0} -&gt; ToTensor starting from (PILImage mode=RGB size=333x500, TensorCategory(14)) applying Resize -- {&#39;size&#39;: (460, 460), &#39;method&#39;: &#39;crop&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;resamples&#39;: (2, 0), &#39;p&#39;: 1.0} gives (PILImage mode=RGB size=460x460, TensorCategory(14)) applying ToTensor gives (TensorImage of size 3x460x460, TensorCategory(14)) Adding the next 3 samples No before_batch transform to apply Collating items in a batch Applying batch_tfms to the batch built Pipeline: IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} -&gt; Flip -- {&#39;size&#39;: None, &#39;mode&#39;: &#39;bilinear&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;mode_mask&#39;: &#39;nearest&#39;, &#39;align_corners&#39;: True, &#39;p&#39;: 0.5} -&gt; RandomResizedCropGPU -- {&#39;size&#39;: (224, 224), &#39;min_scale&#39;: 0.75, &#39;ratio&#39;: (1, 1), &#39;mode&#39;: &#39;bilinear&#39;, &#39;valid_scale&#39;: 1.0, &#39;p&#39;: 1.0} -&gt; Brightness -- {&#39;max_lighting&#39;: 0.2, &#39;p&#39;: 1.0, &#39;draw&#39;: None, &#39;batch&#39;: False} starting from (TensorImage of size 4x3x460x460, TensorCategory([14, 1, 6, 34], device=&#39;cuda:0&#39;)) applying IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} gives (TensorImage of size 4x3x460x460, TensorCategory([14, 1, 6, 34], device=&#39;cuda:0&#39;)) applying Flip -- {&#39;size&#39;: None, &#39;mode&#39;: &#39;bilinear&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;mode_mask&#39;: &#39;nearest&#39;, &#39;align_corners&#39;: True, &#39;p&#39;: 0.5} gives (TensorImage of size 4x3x460x460, TensorCategory([14, 1, 6, 34], device=&#39;cuda:0&#39;)) applying RandomResizedCropGPU -- {&#39;size&#39;: (224, 224), &#39;min_scale&#39;: 0.75, &#39;ratio&#39;: (1, 1), &#39;mode&#39;: &#39;bilinear&#39;, &#39;valid_scale&#39;: 1.0, &#39;p&#39;: 1.0} gives (TensorImage of size 4x3x224x224, TensorCategory([14, 1, 6, 34], device=&#39;cuda:0&#39;)) applying Brightness -- {&#39;max_lighting&#39;: 0.2, &#39;p&#39;: 1.0, &#39;draw&#39;: None, &#39;batch&#39;: False} gives (TensorImage of size 4x3x224x224, TensorCategory([14, 1, 6, 34], device=&#39;cuda:0&#39;)) . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(2) . Downloading: &#34;https://download.pytorch.org/models/resnet34-333f7ec4.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth . . epoch train_loss valid_loss error_rate time . 0 | 1.463172 | 1.101596 | 0.339648 | 01:55 | . epoch train_loss valid_loss error_rate time . 0 | 0.507639 | 1.001056 | 0.294317 | 02:27 | . 1 | 0.330711 | 0.841297 | 0.254398 | 02:27 | . x,y = dls.one_batch() . y . TensorCategory([ 7, 7, 0, 10, 5, 12, 19, 0, 24, 35, 6, 19, 33, 9, 27, 27, 34, 13, 11, 34, 11, 19, 18, 35, 31, 33, 34, 27, 4, 32, 34, 19, 12, 16, 17, 0, 11, 23, 29, 13, 25, 14, 36, 31, 5, 29, 12, 23, 22, 28, 4, 30, 25, 16, 17, 20, 24, 8, 4, 27, 34, 25, 19, 11], device=&#39;cuda:0&#39;) . preds,_ = learn.get_preds(dl=[(x,y)]) preds[0] . tensor([1.3026e-08, 4.1057e-08, 2.1229e-06, 1.3474e-08, 5.7301e-06, 8.5949e-09, 2.8635e-04, 9.9957e-01, 1.0431e-05, 8.2644e-08, 3.1241e-09, 3.8788e-08, 3.1625e-08, 3.9096e-08, 3.8911e-08, 4.1528e-08, 2.3954e-08, 5.3725e-09, 6.2817e-08, 1.8606e-08, 3.3789e-08, 2.4073e-07, 1.0377e-04, 7.2735e-07, 6.6701e-07, 6.4277e-07, 7.1024e-09, 4.2683e-07, 1.2703e-05, 3.6499e-08, 4.7538e-08, 1.7708e-06, 7.3165e-08, 2.0073e-07, 1.9836e-08, 4.8690e-07, 6.4921e-07]) . len(preds[0]),preds[0].sum() . (37, tensor(1.0000)) . plot_function(torch.sigmoid, min=-4,max=4) . /usr/local/lib/python3.7/dist-packages/fastbook/__init__.py:73: UserWarning: Not providing a value for linspace&#39;s steps is deprecated and will throw a runtime error in a future release. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/RangeFactories.cpp:23.) x = torch.linspace(min,max) . torch.random.manual_seed(42); . acts = torch.randn((6,2))*2 acts . tensor([[ 0.6734, 0.2576], [ 0.4689, 0.4607], [-2.2457, -0.3727], [ 4.4164, -1.2760], [ 0.9233, 0.5347], [ 1.0698, 1.6187]]) . acts.sigmoid() . tensor([[0.6623, 0.5641], [0.6151, 0.6132], [0.0957, 0.4079], [0.9881, 0.2182], [0.7157, 0.6306], [0.7446, 0.8346]]) . (acts[:,0]-acts[:,1]).sigmoid() . tensor([0.6025, 0.5021, 0.1332, 0.9966, 0.5959, 0.3661]) . sm_acts = torch.softmax(acts, dim=1) sm_acts . tensor([[0.6025, 0.3975], [0.5021, 0.4979], [0.1332, 0.8668], [0.9966, 0.0034], [0.5959, 0.4041], [0.3661, 0.6339]]) . targ = tensor([0,1,0,1,1,0]) . sm_acts . tensor([[0.6025, 0.3975], [0.5021, 0.4979], [0.1332, 0.8668], [0.9966, 0.0034], [0.5959, 0.4041], [0.3661, 0.6339]]) . idx = range(6) sm_acts[idx, targ] . tensor([0.6025, 0.4979, 0.1332, 0.0034, 0.4041, 0.3661]) . from IPython.display import HTML df = pd.DataFrame(sm_acts, columns=[&quot;3&quot;,&quot;7&quot;]) df[&#39;targ&#39;] = targ df[&#39;idx&#39;] = idx df[&#39;loss&#39;] = sm_acts[range(6), targ] t = df.style.hide_index() #To have html code compatible with our script html = t._repr_html_().split(&#39;&lt;/style&gt;&#39;)[1] html = re.sub(r&#39;&lt;table id=&quot;([^&quot;]+)&quot; s*&gt;&#39;, r&#39;&lt;table &gt;&#39;, html) display(HTML(html)) . 3 7 targ idx loss . 0.602469 | 0.397531 | 0 | 0 | 0.602469 | . 0.502065 | 0.497935 | 1 | 1 | 0.497935 | . 0.133188 | 0.866811 | 0 | 2 | 0.133188 | . 0.996640 | 0.003360 | 1 | 3 | 0.003360 | . 0.595949 | 0.404051 | 1 | 4 | 0.404051 | . 0.366118 | 0.633882 | 0 | 5 | 0.366118 | . -sm_acts[idx, targ] . tensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661]) . F.nll_loss(sm_acts, targ, reduction=&#39;none&#39;) . tensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661]) . plot_function(torch.log, min=0,max=4) . loss_func = nn.CrossEntropyLoss() . F.cross_entropy(acts, targ) . tensor(1.8045) . nn.CrossEntropyLoss(reduction=&#39;none&#39;)(acts, targ) . tensor([0.5067, 0.6973, 2.0160, 5.6958, 0.9062, 1.0048]) . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix(figsize=(12,12), dpi=60) . interp.most_confused(min_val=5) . [(&#39;Maine_Coon&#39;, &#39;Ragdoll&#39;, 15), (&#39;Russian_Blue&#39;, &#39;British_Shorthair&#39;, 13), (&#39;Birman&#39;, &#39;Ragdoll&#39;, 12), (&#39;beagle&#39;, &#39;basset_hound&#39;, 11), (&#39;yorkshire_terrier&#39;, &#39;havanese&#39;, 10), (&#39;staffordshire_bull_terrier&#39;, &#39;american_pit_bull_terrier&#39;, 9), (&#39;Siamese&#39;, &#39;Birman&#39;, 8), (&#39;english_setter&#39;, &#39;english_cocker_spaniel&#39;, 8), (&#39;keeshond&#39;, &#39;samoyed&#39;, 8), (&#39;Maine_Coon&#39;, &#39;Persian&#39;, 7), (&#39;staffordshire_bull_terrier&#39;, &#39;american_bulldog&#39;, 7), (&#39;wheaten_terrier&#39;, &#39;havanese&#39;, 7), (&#39;Siamese&#39;, &#39;Bombay&#39;, 6), (&#39;chihuahua&#39;, &#39;miniature_pinscher&#39;, 6), (&#39;english_setter&#39;, &#39;basset_hound&#39;, 6), (&#39;Abyssinian&#39;, &#39;Sphynx&#39;, 5), (&#39;american_bulldog&#39;, &#39;american_pit_bull_terrier&#39;, 5), (&#39;beagle&#39;, &#39;american_bulldog&#39;, 5), (&#39;chihuahua&#39;, &#39;Sphynx&#39;, 5), (&#39;leonberger&#39;, &#39;newfoundland&#39;, 5), (&#39;saint_bernard&#39;, &#39;american_bulldog&#39;, 5)] . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(1, base_lr=0.1) . epoch train_loss valid_loss error_rate time . 0 | 2.641252 | 10.990454 | 0.837618 | 01:56 | . epoch train_loss valid_loss error_rate time . 0 | 3.699024 | 3.996556 | 0.953992 | 02:29 | . learn = cnn_learner(dls, resnet34, metrics=error_rate) lr_min,lr_steep = learn.lr_find() . print(f&quot;Minimum/10: {lr_min:.2e}, steepest point: {lr_steep:.2e}&quot;) . Minimum/10: 1.00e-02, steepest point: 3.63e-03 . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(2, base_lr=3e-3) . epoch train_loss valid_loss error_rate time . 0 | 1.347023 | 0.981549 | 0.288227 | 01:55 | . epoch train_loss valid_loss error_rate time . 0 | 0.566394 | 1.174068 | 0.330176 | 02:29 | . 1 | 0.334773 | 0.752081 | 0.225304 | 02:29 | . learn.fine_tune?? . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fit_one_cycle(3, 3e-3) . epoch train_loss valid_loss error_rate time . 0 | 1.104290 | 0.933496 | 0.271989 | 01:56 | . 1 | 0.547801 | 0.765140 | 0.230717 | 01:55 | . 2 | 0.327906 | 0.682352 | 0.209743 | 01:55 | . learn.unfreeze() . learn.lr_find() . SuggestedLRs(lr_min=9.999999747378752e-07, lr_steep=2.2908675418875646e-06) . learn.fit_one_cycle(6, lr_max=1e-5) . epoch train_loss valid_loss error_rate time . 0 | 0.258392 | 0.652376 | 0.200947 | 02:27 | . 1 | 0.242147 | 0.618082 | 0.189445 | 02:27 | . 2 | 0.231463 | 0.622283 | 0.196888 | 02:27 | . 3 | 0.204535 | 0.595082 | 0.188092 | 02:27 | . 4 | 0.180945 | 0.592548 | 0.186062 | 02:27 | . 5 | 0.171708 | 0.603632 | 0.192828 | 02:28 | . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fit_one_cycle(3, 3e-3) learn.unfreeze() learn.fit_one_cycle(12, lr_max=slice(1e-6,1e-4)) . epoch train_loss valid_loss error_rate time . 0 | 1.124157 | 0.991350 | 0.308525 | 01:55 | . 1 | 0.527673 | 0.893586 | 0.282138 | 01:55 | . 2 | 0.339577 | 0.801603 | 0.253721 | 01:54 | . epoch train_loss valid_loss error_rate time . 0 | 0.272625 | 0.764421 | 0.238836 | 02:26 | . 1 | 0.260812 | 0.761544 | 0.242896 | 02:26 | . 2 | 0.229428 | 0.696853 | 0.222598 | 02:26 | . 3 | 0.226262 | 0.675862 | 0.219215 | 02:29 | . 4 | 0.189527 | 0.686940 | 0.220568 | 02:29 | . 5 | 0.173371 | 0.682683 | 0.218539 | 02:29 | . 6 | 0.165010 | 0.650099 | 0.208390 | 02:29 | . 7 | 0.161229 | 0.651157 | 0.209743 | 02:29 | . 8 | 0.138687 | 0.640442 | 0.203654 | 02:29 | . 9 | 0.127961 | 0.651706 | 0.202300 | 02:29 | . 10 | 0.124051 | 0.615696 | 0.193505 | 02:29 | . 11 | 0.124554 | 0.640680 | 0.200947 | 02:29 | . learn.recorder.plot_loss() . from fastai.callback.fp16 import * learn = cnn_learner(dls, resnet50, metrics=error_rate).to_fp16() learn.fine_tune(6, freeze_epochs=3) . Downloading: &#34;https://download.pytorch.org/models/resnet50-19c8e357.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth . . epoch train_loss valid_loss error_rate time . 0 | 1.264476 | 1.317010 | 0.381597 | 02:17 | . 1 | 0.602886 | 1.197397 | 0.348444 | 02:16 | . 2 | 0.430401 | 1.302739 | 0.358593 | 02:16 | . epoch train_loss valid_loss error_rate time . 0 | 0.267705 | 1.193918 | 0.342355 | 02:57 | . 1 | 0.333476 | 1.369267 | 0.364005 | 02:57 | . 2 | 0.258500 | 1.029740 | 0.303112 | 02:57 | . 3 | 0.147498 | 0.883164 | 0.253721 | 02:57 | . 4 | 0.085893 | 0.760042 | 0.224628 | 02:57 | . 5 | 0.052952 | 0.767537 | 0.227334 | 02:56 | .",
            "url": "https://imotep460.github.io/FastAIBlog/2021/03/09/Kapitel-5-FastAI.html",
            "relUrl": "/2021/03/09/Kapitel-5-FastAI.html",
            "date": " • Mar 9, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Title",
            "content": ". !pip install -Uqq fastbook import fastbook fastbook.setup_book() . |████████████████████████████████| 727kB 5.9MB/s |████████████████████████████████| 194kB 33.0MB/s |████████████████████████████████| 51kB 5.3MB/s |████████████████████████████████| 1.2MB 29.8MB/s |████████████████████████████████| 61kB 7.6MB/s Mounted at /content/gdrive . from fastbook import * from fastai.vision.widgets import * . configId = &quot;&quot; . subscriptionKey = &quot;&quot; . def search_images_bing_new(key, term, customConfigId, min_sz=128): url = &#39;https://api.bing.microsoft.com/v7.0/custom/images/search?&#39; + &#39;q=&#39; + term + &#39;&amp;&#39; + &#39;customconfig=&#39; + customConfigId + &#39;&amp;&#39; + &#39;count=150&#39; r = requests.get(url, headers={&#39;Ocp-Apim-Subscription-Key&#39;: key}) search_results = r.json() return L([img[&quot;thumbnailUrl&quot;] + &quot;.jpg&quot; for img in search_results[&quot;value&quot;][:150]]) . hotdogImages = search_images_bing_new(subscriptionKey, &quot;hotdog&quot;, configId) . firstHotdogImage = hotdogImages[0] hotdogDest = &quot;hotdog.jpg&quot; download_url(firstHotdogImage, hotdogDest) . hotdogImg = Image.open(hotdogDest) hotdogImg.to_thumb(128,128) . imagesPath = Path(&#39;images&#39;) if not imagesPath.exists: imagesPath.mkdir() . hotdogPath = Path(str(imagesPath) + &#39;/hotdogs&#39;) if not hotdogPath.exists(): hotdogPath.mkdir() download_images(hotdogPath, urls = hotdogImages) . hotdogImageFiles = get_image_files(hotdogPath) . failedHotdogs = verify_images(hotdogImageFiles) failedHotdogs . (#0) [] . foodImages = search_images_bing_new(subscriptionKey, &quot;food -hotdog&quot;, configId) foodPath = Path(str(imagesPath) + &#39;/food&#39;) if not foodPath.exists(): foodPath.exists() download_images(foodPath, urls=foodImages) foodImageFiles = get_image_files(foodPath) failedFood = verify_images(foodImageFiles) failedFood . (#0) [] . foods = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(128)) foods = foods.new( item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms()) . dls = foods.dataloaders(imagesPath) . dls.valid.show_batch(max_n=12, nrows=1) . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(8) . Downloading: &#34;https://download.pytorch.org/models/resnet18-5c106cde.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth . . epoch train_loss valid_loss error_rate time . 0 | 1.251222 | 0.799934 | 0.333333 | 00:03 | . epoch train_loss valid_loss error_rate time . 0 | 0.697134 | 0.601997 | 0.266667 | 00:03 | . 1 | 0.613012 | 0.463671 | 0.183333 | 00:03 | . 2 | 0.512704 | 0.417398 | 0.133333 | 00:03 | . 3 | 0.414984 | 0.428409 | 0.133333 | 00:03 | . 4 | 0.356606 | 0.449707 | 0.150000 | 00:03 | . 5 | 0.310091 | 0.449083 | 0.100000 | 00:03 | . 6 | 0.286895 | 0.445496 | 0.083333 | 00:03 | . 7 | 0.256358 | 0.448062 | 0.083333 | 00:03 | . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . learn.export() . Image.open(&quot;hotdog.jpg&quot;).to_thumb(128,128) . pred,pred_idx,probs = learn.predict(&quot;hotdog.jpg&quot;) f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; . &#39;Prediction: hotdogs; Probability: 0.9990&#39; . Image.open(&quot;burger.jpg&quot;).to_thumb(128,128) . pred,pred_idx,probs = learn.predict(&quot;burger.jpg&quot;) f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; . &#39;Prediction: food; Probability: 0.9995&#39; . Image.open(&quot;dog.jpg&quot;).to_thumb(128,128) . pred,pred_idx,probs = learn.predict(&quot;dog.jpg&quot;) f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; . &#39;Prediction: food; Probability: 0.9954&#39; . Image.open(&quot;hotdogdog.jpg&quot;).to_thumb(128,128) . pred,pred_idx,probs = learn.predict(&quot;hotdogdog.jpg&quot;) f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; . &#39;Prediction: hotdogs; Probability: 0.8661&#39; .",
            "url": "https://imotep460.github.io/FastAIBlog/2021/02/16/Kapitel-2-hotdogs.html",
            "relUrl": "/2021/02/16/Kapitel-2-hotdogs.html",
            "date": " • Feb 16, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://imotep460.github.io/FastAIBlog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://imotep460.github.io/FastAIBlog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://imotep460.github.io/FastAIBlog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://imotep460.github.io/FastAIBlog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}